{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "translate.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Using TensorFlow backend.\ntrain\ntest\n"
        }
      ],
      "source": [
        "from pickle import load\n",
        "from numpy import argmax\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "    return load(open(filename, 'rb'))\n",
        "\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(lines)\n",
        "    return tokenizer\n",
        "\n",
        "\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "    return max(len(line.split()) for line in lines)\n",
        "\n",
        "\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "    # integer encode sequences\n",
        "    X = tokenizer.texts_to_sequences(lines)\n",
        "    # pad sequences with 0 values\n",
        "    X = pad_sequences(X, maxlen=length, padding='post')\n",
        "    return X\n",
        "\n",
        "\n",
        "# map an integer to a word\n",
        "def word_for_id(integer, tokenizer):\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == integer:\n",
        "            return word\n",
        "    return None\n",
        "\n",
        "\n",
        "# generate target given source sequence\n",
        "def predict_sequence(model, tokenizer, source):\n",
        "    prediction = model.predict(source, verbose=0)[0]\n",
        "    integers = [argmax(vector) for vector in prediction]\n",
        "    target = list()\n",
        "    for i in integers:\n",
        "        word = word_for_id(i, tokenizer)\n",
        "        if word is None:\n",
        "            break\n",
        "        target.append(word)\n",
        "    return ' '.join(target)\n",
        "\n",
        "\n",
        "# evaluate the skill of the model\n",
        "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
        "    actual, predicted = list(), list()\n",
        "    for i, source in enumerate(sources):\n",
        "        # translate encoded source text\n",
        "        source = source.reshape((1, source.shape[0]))\n",
        "        translation = predict_sequence(model, eng_tokenizer, source)\n",
        "        raw_target, raw_src, test = raw_dataset[i]\n",
        "        '''if i < 10:\n",
        "            print('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
        "        actual.append([raw_target.split()])\n",
        "        predicted.append(translation.split())\n",
        "    # calculate BLEU score\n",
        "    print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
        "    print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
        "    print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
        "    print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))'''\n",
        "\n",
        "\n",
        "# load datasets\n",
        "dataset = load_clean_sentences('english-kannada-both36.pkl')\n",
        "train = load_clean_sentences('english-kannada-train36.pkl')\n",
        "test = load_clean_sentences('english-kannada-test36.pkl')\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "# prepare kannada tokenizer\n",
        "kan_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "kan_vocab_size = len(kan_tokenizer.word_index) + 1\n",
        "kan_length = max_length(dataset[:, 1])\n",
        "# prepare data\n",
        "trainX = encode_sequences(kan_tokenizer, kan_length, train[:, 1])\n",
        "testX = encode_sequences(kan_tokenizer, kan_length, test[:, 1])\n",
        "\n",
        "\n",
        "# load model\n",
        "model = load_model('model32k.h5')\n",
        "# test on some training sequences\n",
        "print('train')\n",
        "evaluate_model(model, eng_tokenizer, trainX, train)\n",
        "# test on some test sequences\n",
        "print('test')\n",
        "evaluate_model(model, eng_tokenizer, testX, test)\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "colab_type": "code",
        "id": "CxCIaNLcnD7j",
        "outputId": "21cc4f5d-0328-4a5d-e880-cbf8b1249765"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "predicted == i   i\n"
        }
      ],
      "source": [
        "def custom_pred(sentence):\n",
        "    data = [sentence]\n",
        "    data = kan_tokenizer.texts_to_sequences(data)\n",
        "    data = pad_sequences(data, maxlen=kan_length, padding='post')\n",
        "    data = predict_sequence(model, eng_tokenizer, data)\n",
        "    data = \" \".join(data[:])\n",
        "    return data\n",
        "\n",
        "\n",
        "inp = input(\"ಕನ್ನಡ ವಾಕ್ಯವನ್ನು ನಮೂದಿಸಿ:::----\")\n",
        "a = custom_pred(inp)\n",
        "print(\"predicted ==\", a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ]
}